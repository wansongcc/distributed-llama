#include "app.hpp"
#include "llm.hpp" // åŒ…å« loadLlmNetWeightUneven, buildLlmNetUneven ç­‰å£°æ˜
#include "nn/nn-network.hpp"
#include "nn/nn-network-local.hpp" // [æ–°å¢] æœ¬åœ°åŠ è½½å™¨
#include <cassert>
#include <cstring>
#include <sstream>
#include <numeric>
#include <cmath>
#include <vector>
#include <stdexcept>

#if defined(DLLAMA_VULKAN)
    #include "nn/nn-vulkan.hpp"
#endif

// --- è¾…åŠ©å‡½æ•° ---

static NnFloatType parseFloatType(char *val) {
    if (std::strcmp(val, "f32") == 0) return F_32;
    if (std::strcmp(val, "f16") == 0) return F_16;
    if (std::strcmp(val, "q40") == 0) return F_Q40;
    if (std::strcmp(val, "q80") == 0) return F_Q80;
    throw std::runtime_error("Invalid float type: " + std::string(val));
}

static ChatTemplateType parseChatTemplateType(char *val) {
    if (std::strcmp(val, "llama2") == 0) return TEMPLATE_LLAMA2;
    if (std::strcmp(val, "llama3") == 0) return TEMPLATE_LLAMA3;
    if (std::strcmp(val, "deepSeek3") == 0) return TEMPLATE_DEEP_SEEK3;
    throw std::runtime_error("Invalid chat template type: " + std::string(val));
}

// [æ–°å¢] è§£æé€—å·åˆ†éš”çš„æ¯”ä¾‹å­—ç¬¦ä¸²
static std::vector<float> parseRatios(const char *ratiosStr, NnUint nNodes) {
    if (ratiosStr == nullptr) {
        throw std::invalid_argument("Ratios string cannot be empty");
    }
    std::vector<float> ratios;
    std::string s(ratiosStr);
    std::stringstream ss(s);
    std::string item;
    while (std::getline(ss, item, ',')) {
        if (!item.empty()) {
            try {
                ratios.push_back(std::stof(item));
            } catch (...) {
                throw std::invalid_argument(std::string("Invalid ratio value: ") + item);
            }
        }
    }
    if (ratios.size() != nNodes) {
        throw std::invalid_argument("Number of ratios (" + std::to_string(ratios.size()) + 
                                  ") must match number of nodes (" + std::to_string(nNodes) + ")");
    }
    return ratios;
}

// --- AppCliArgs å®ç° ---

AppCliArgs AppCliArgs::parse(int argc, char* *argv, bool requireMode) {
    AppCliArgs args;
    args.info = true;
    args.help = false;
    args.mode = nullptr;
    args.nBatches = 32;
    args.nThreads = 1;
    args.modelPath = nullptr;
    args.tokenizerPath = nullptr;
    args.prompt = nullptr;
    args.syncType = F_32;
    args.nWorkers = 0;
    args.workerHosts = nullptr;
    args.workerPorts = nullptr;
    args.port = 9990;
    args.temperature = 0.8f;
    args.topp = 0.9f;
    args.steps = 0;
    args.seed = (unsigned long long)time(nullptr);
    args.chatTemplateType = TEMPLATE_UNKNOWN;
    args.maxSeqLen = 0;
    args.netTurbo = true;
    args.gpuIndex = -1;
    args.gpuSegmentFrom = -1;
    args.gpuSegmentTo = -1;
    args.ratiosStr = nullptr;

    int i = 1;
    if (requireMode && argc > 1) {
        args.mode = argv[1];
        i++;
    }
    for (int x = 0; x < argc; x++) {
        if ((std::strcmp(argv[x], "--usage") == 0) ||
            (std::strcmp(argv[x], "--help") == 0) ||
            (std::strcmp(argv[x], "-h") == 0)) {
            args.help = true;
            return args;
        }
    }
    for (; i + 1 < argc; i += 2) {
        char *name = argv[i];
        char *value = argv[i + 1];
        if (std::strcmp(name, "--model") == 0) {
            args.modelPath = value;
        } else if (std::strcmp(name, "--tokenizer") == 0) {
            args.tokenizerPath = value;
        } else if (std::strcmp(name, "--prompt") == 0) {
            args.prompt = value;
        } else if (std::strcmp(name, "--buffer-float-type") == 0) {
            args.syncType = parseFloatType(value);
        } else if (std::strcmp(name, "--workers") == 0) {
            int j = i + 1;
            for (; j < argc && argv[j][0] != '-'; j++);
            int count = j - i - 1;

            args.nWorkers = count;
            args.workerHosts = new char*[count];
            args.workerPorts = new NnUint[count];

            for (int s = 0; s < count; s++) {
                char *v = argv[i + 1 + s];
                char *separator = std::strstr(v, ":");
                if (separator == NULL) {
                    throw std::runtime_error("Invalid worker address: " + std::string(v));
                }
                int hostLen = separator - v;
                args.workerHosts[s] = new char[hostLen + 1];
                std::memcpy(args.workerHosts[s], v, hostLen);
                args.workerHosts[s][hostLen] = '\0';
                args.workerPorts[s] = std::atoi(separator + 1);
            }
            i += count - 1;
        } else if (std::strcmp(name, "--ratios") == 0) {
            // ä¿®å¤ä¹‹å‰çš„ bugï¼šä¸ä½¿ç”¨ ++iï¼Œç›´æ¥ä½¿ç”¨ value
            args.ratiosStr = value;
        } else if (std::strcmp(name, "--port") == 0) {
            args.port = atoi(value);
        } else if (std::strcmp(name, "--nthreads") == 0) {
            args.nThreads = atoi(value);
        } else if (std::strcmp(name, "--steps") == 0) {
            args.steps = atoi(value);
        } else if (std::strcmp(name, "--temperature") == 0) {
            args.temperature = atof(value);
        } else if (std::strcmp(name, "--topp") == 0) {
            args.topp = atof(value);
        } else if (std::strcmp(name, "--seed") == 0) {
            args.seed = atoll(value);
        } else if (std::strcmp(name, "--chat-template") == 0) {
            args.chatTemplateType = parseChatTemplateType(value);
        } else if (std::strcmp(name, "--max-seq-len") == 0) {
            args.maxSeqLen = (unsigned int)atoi(value);
        } else if (std::strcmp(name, "--gpu-index") == 0) {
            args.gpuIndex = atoi(value);
        } else if (std::strcmp(name, "--gpu-segments") == 0) {
            char *separator = std::strstr(value, ":");
            if (separator == NULL)
                throw std::runtime_error("GPU segments expected in the format <from>:<to>");
            args.gpuSegmentFrom = atoi(value);
            args.gpuSegmentTo = atoi(separator + 1);
        } else if (std::strcmp(name, "--net-turbo") == 0) {
            args.netTurbo = atoi(value) == 1;
        } else {
            throw std::runtime_error("Unknown option: " + std::string(name));
        }
    }

    if (args.nThreads < 1)
        throw std::runtime_error("Number of threads must be at least 1");
    return args;
}

AppCliArgs::~AppCliArgs() {
    if (workerHosts != nullptr) {
        for (NnUint i = 0; i < nWorkers; i++)
            delete[] workerHosts[i];
        delete[] workerHosts;
    }
    if (workerPorts != nullptr)
        delete[] workerPorts;
}

// --- Device Resolution ---

static std::vector<NnExecutorDevice> resolveDevices(AppCliArgs *args, NnNetConfig *netConfig, NnNodeConfig *nodeConfig, NnNetExecution *netExecution, const NnUnevenPartitionPlan *plan = nullptr) {
    std::vector<NnExecutorDevice> devices;

    if (args->gpuIndex >= 0) {
#if defined(DLLAMA_VULKAN)
        devices.push_back(NnExecutorDevice(
            new NnVulkanDevice(args->gpuIndex, netConfig, nodeConfig, netExecution),
            args->gpuSegmentFrom,
            args->gpuSegmentTo
        ));
#else
        throw std::runtime_error("This build does not support GPU");
#endif
    }

    if (args->gpuIndex < 0 || (args->gpuSegmentFrom >= 0 && args->gpuSegmentTo >= 0)) {
        // ä¼ å…¥ plan ä»¥æ”¯æŒéå‡åŒ€åˆ‡åˆ†æ—¶çš„ç¨³å®šæ€§æ£€æŸ¥å’ŒæŒ‡é’ˆè®¡ç®—
        devices.push_back(NnExecutorDevice(new NnCpuDevice(netConfig, nodeConfig, netExecution, plan), -1, -1));
    }
    return devices;
}

// --- Inference Implementations (Root & Worker) ---

RootLlmInference::RootLlmInference(LlmNet *net, NnNetExecution *execution, NnExecutor *executor, NnNetwork *network) {
    this->header = net->header;
    this->tokenPipe = (float *)execution->pipes[net->tokenPipeIndex];
    this->positionPipe = (float *)execution->pipes[net->positionPipeIndex];
    this->logitsPipe = (float *)execution->pipes[net->logitsPipeIndex];
    this->execution = execution;
    this->executor = executor;
    this->network = network; 
}

void RootLlmInference::setBatchSize(NnUint batchSize) {
    execution->setBatchSize(batchSize);
    controlPacket.batchSize = batchSize;
}

void RootLlmInference::setPosition(NnUint position) {
    assert(position >= 0);
    assert(position + execution->batchSize - 1 < header->seqLen);

    controlPacket.position = position;
    for (NnUint i = 0; i < execution->batchSize; i++)
        positionPipe[i] = (float)(position + i);
}

void RootLlmInference::setToken(NnUint batchIndex, NnUint token) {
    assert(batchIndex >= 0 && batchIndex < execution->batchSize);
    tokenPipe[batchIndex] = (float)token;
}

void RootLlmInference::forward() {
    if (network != nullptr) 
        network->writeAll(&controlPacket, sizeof(LlmControlPacket));
    executor->forward();
}

void RootLlmInference::finish() {
    if (network != nullptr) {
        controlPacket.batchSize = 0;
        network->writeAll(&controlPacket, sizeof(LlmControlPacket));
    }
}

WorkerLlmInference::WorkerLlmInference(NnNetExecution *execution, NnNetwork *network) {
    this->isFinished = false;
    this->execution = execution;
    this->network = network;
    this->positionPipe = (float *)execution->pipes[0];
}

bool WorkerLlmInference::tryReadControlPacket() {
    const unsigned long maxAttempts = 10000;
    if (!network->tryReadWithMaxAttempts(ROOT_SOCKET_INDEX, &controlPacket, sizeof(LlmControlPacket), maxAttempts))
        return false;
    if (controlPacket.batchSize == 0) {
        printf("ğŸ›‘ Stop signal\n");
        isFinished = true;
        return true;
    }
    for (NnUint i = 0; i < controlPacket.batchSize; i++)
        positionPipe[i] = (float)(controlPacket.position + i);
    execution->setBatchSize(controlPacket.batchSize);
    return true;
}

// --- Main Application Entry Points ---

void runInferenceApp(AppCliArgs *args, void (*handler)(AppInferenceContext *context)) {
    NnUint nNodes = args->nWorkers + 1;

    LlmHeader header = loadLlmHeader(args->modelPath, args->maxSeqLen, args->syncType);
    if (nNodes > header.nKvHeads)
        // TODO: https://github.com/b4rtaz/distributed-llama/issues/70
        throw std::runtime_error("This version does not support more nodes than the number of KV heads in the model");
    if (header.weightType == F_Q40 && header.syncType != F_Q80)
        throw std::runtime_error("This version supports only Q40 weights with Q80 sync type");

    Tokenizer tokenizer(args->tokenizerPath);
    if (args->info && tokenizer.vocabSize != header.vocabSize)
        printf("Tokenizer vocab size (%d) does not match the model vocab size (%d)\n", tokenizer.vocabSize, header.vocabSize);

    Sampler sampler(tokenizer.vocabSize, args->temperature, args->topp, args->seed);

    // --- 1. Build Network ---
    LlmNet net;
    std::vector<float> ratios;
    // ç”¨äºä¿å­˜ plan æŒ‡é’ˆï¼Œä»¥ä¾¿ä¼ é€’ç»™ resolveDevices
    std::unique_ptr<NnUnevenPartitionPlan> planPtr; 

    if (args->ratiosStr != nullptr) {
        // [éå‡åŒ€æ¨¡å¼]
        printf("ğŸš€ Mode: Uneven Partitioning (%s)\n", args->ratiosStr);
        ratios = parseRatios(args->ratiosStr, nNodes);
        net = buildLlmNetUneven(&header, nNodes, args->nBatches, ratios);
    } else {
        // [å‡åŒ€æ¨¡å¼]
        printf("ğŸš€ Mode: Uniform Partitioning\n");
        net = buildLlmNet(&header, nNodes, args->nBatches);
    }

    std::unique_ptr<LlmNet, void(*)(LlmNet *)> netPtr(&net, releaseLlmNet);
    NnNodeConfig *rootNodeConfig = &net.nodeConfigs[0];

    if (args->info) {
        tokenizer.printHeader();
        printLlmHeader(&header);
        printNodeRequiredMemory(&net.netConfig, rootNodeConfig);
    }

    NnNetExecution execution(args->nThreads, &net.netConfig);
    std::unique_ptr<NnNodeSynchronizer> synchronizer(nullptr);
    std::unique_ptr<NnNetwork> networkPtr(nullptr);
    NnNetwork *network = nullptr;

    if (nNodes == 1) {
        synchronizer.reset(new NnFakeNodeSynchronizer());
    } else {
        networkPtr = NnNetwork::connect(args->nWorkers, args->workerHosts, args->workerPorts);
        network = networkPtr.get();
        synchronizer.reset(new NnNetworkNodeSynchronizer(network, &execution, &net.netConfig, rootNodeConfig));

        NnRootConfigWriter configWriter(network);
        configWriter.writeToWorkers(&net.netConfig, net.nodeConfigs);
    }

    // --- 2. Initialize Executor ---
    // å¦‚æœæ˜¯éå‡åŒ€æ¨¡å¼ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»º Plan å¹¶ä¼ é€’ç»™ resolveDevices
    if (args->ratiosStr != nullptr) {
        NnUint ffDim = (header.archType == QWEN3_MOE) ? header.moeHiddenDim : header.hiddenDim;
        planPtr.reset(new NnUnevenPartitionPlan(
            createPartitionPlan(nNodes, ratios, header.nHeads, header.nKvHeads, header.vocabSize, ffDim)
        ));
    }
    
    std::vector<NnExecutorDevice> devices = resolveDevices(args, &net.netConfig, rootNodeConfig, &execution, planPtr.get());
    NnExecutor executor(&net.netConfig, rootNodeConfig, &devices, &execution, synchronizer.get(), args->benchmark);

    // --- 3. Load Weights ---
    if (args->ratiosStr != nullptr) {
        // [éå‡åŒ€ + æœ¬åœ°åŠ è½½]
        printf("ğŸš€ Local Loading Mode (Root): Loading weights locally...\n");
        
        // åˆ›å»ºæœ¬åœ°åŠ è½½å™¨ (Node 0)
        NnLocalWeightLoader localLoader(&executor, 0); 
        
        // åŠ è½½
        loadLlmNetWeightUneven(args->modelPath, &net, &localLoader, planPtr.get());
        
    } else {
        // [å‡åŒ€ + ç½‘ç»œåˆ†å‘] (Legacy)
        NnRootWeightLoader weightLoader(&executor, network, nNodes);
        loadLlmNetWeight(args->modelPath, &net, &weightLoader);
    }

    // --- 4. Inference ---
    RootLlmInference inference(&net, &execution, &executor, network);

    if (network != nullptr) {
        network->resetStats();
        if (args->netTurbo) {
            network->setTurbo(true);
            printf("ğŸš Network is in non-blocking mode\n");
        }
    }

    AppInferenceContext context;
    context.args = args;
    context.header = &header;
    context.inference = &inference;
    context.sampler = &sampler;
    context.tokenizer = &tokenizer;
    context.network = network;
    context.executor = &executor;

    handler(&context);

    inference.finish();
    
    // Plan ç”± unique_ptr è‡ªåŠ¨é‡Šæ”¾
}

void runWorkerApp(AppCliArgs *args) {
    while (true) {
        std::unique_ptr<NnNetwork> networkPtr = NnNetwork::serve(args->port);
        NnNetwork *network = networkPtr.get();

        NnWorkerConfigReader configReader(network);
        NnNetConfig netConfig = configReader.readNet();
        NnNodeConfig nodeConfig = configReader.readNode();
        std::unique_ptr<NnNetConfig, void(*)(NnNetConfig *)> netConfigPtr(&netConfig, releaseNetConfig);
        std::unique_ptr<NnNodeConfig, void(*)(NnNodeConfig *)> nodeConfigPtr(&nodeConfig, releaseNodeConfig);

        printNodeRequiredMemory(&netConfig, &nodeConfig);

        NnNetExecution execution(args->nThreads, &netConfig);
        
        // å‡†å¤‡ Plan (å¦‚æœæ˜¯æœ¬åœ°åŠ è½½æ¨¡å¼) ç”¨äº resolveDevices
        std::unique_ptr<NnUnevenPartitionPlan> planPtr;
        if (args->ratiosStr != nullptr && args->modelPath != nullptr) {
             LlmHeader header = loadLlmHeader(args->modelPath, args->maxSeqLen, args->syncType);
             std::vector<float> ratios = parseRatios(args->ratiosStr, netConfig.nNodes);
             NnUint ffDim = (header.archType == QWEN3_MOE) ? header.moeHiddenDim : header.hiddenDim;
             
             planPtr.reset(new NnUnevenPartitionPlan(
                 createPartitionPlan(netConfig.nNodes, ratios, header.nHeads, header.nKvHeads, header.vocabSize, ffDim)
             ));
        }

        std::vector<NnExecutorDevice> devices = resolveDevices(args, &netConfig, &nodeConfig, &execution, planPtr.get());
        NnNetworkNodeSynchronizer synchronizer(network, &execution, &netConfig, &nodeConfig);
        NnExecutor executor(&netConfig, &nodeConfig, &devices, &execution, &synchronizer, false);

        // --- Weight Loading Logic ---
        if (args->ratiosStr != nullptr && args->modelPath != nullptr) {
            // [æœ¬åœ°åŠ è½½]
            printf("ğŸš€ Worker %d: Local Loading Mode from %s\n", nodeConfig.nodeIndex, args->modelPath);
            
            // é‡æ–°åŠ è½½å¤´ä¿¡æ¯ç”¨äºæ„å»ºä¸´æ—¶ Net
            LlmHeader header = loadLlmHeader(args->modelPath, args->maxSeqLen, args->syncType);
            std::vector<float> ratios = parseRatios(args->ratiosStr, netConfig.nNodes);
            
            // ä¿®æ­£è¾…åŠ©ç»´åº¦
            if (header.headDim == 0 && header.nHeads > 0) header.headDim = header.dim / header.nHeads;
            header.qDim = header.nHeads * header.headDim;
            header.kvDim = header.nKvHeads * header.headDim;

            // æ„å»ºä¸´æ—¶ Net
            LlmNet tempNet = buildLlmNetUneven(&header, netConfig.nNodes, 1, ratios);

            // æ‰§è¡Œæœ¬åœ°åŠ è½½
            NnLocalWeightLoader localLoader(&executor, nodeConfig.nodeIndex);
            loadLlmNetWeightUneven(args->modelPath, &tempNet, &localLoader, planPtr.get());

            releaseLlmNet(&tempNet);
            printf("âœ… Worker %d: Weights loaded locally.\n", nodeConfig.nodeIndex);

        } else {
            // [ç½‘ç»œåŠ è½½] (Legacy)
            printf("ğŸ“¡ Worker %d: Waiting for weights from Root...\n", nodeConfig.nodeIndex);
            NnWorkerWeightReader weightReader(&executor, network);
            weightReader.read();
        }

        WorkerLlmInference inference(&execution, network);
        bool isFirstAttempt = true;
        bool isTurboEnabled = false;
        clock_t startTime;
        while (true) {
            try {
                if (isFirstAttempt)
                    startTime = clock();

                if (!inference.tryReadControlPacket()) {
                    if (isTurboEnabled && !isFirstAttempt && clock() - startTime > CLOCKS_PER_SEC) {
                        network->setTurbo(false);
                        isTurboEnabled = false;
                        printf("ğŸš Network is in blocking mode\n");
                    }
                    isFirstAttempt = false;
                    continue;
                }
                if (inference.isFinished)
                    break;

                if (args->netTurbo && !isTurboEnabled) {
                    network->setTurbo(true);
                    isTurboEnabled = true;
                    printf("ğŸš Network is in non-blocking mode\n");
                }
                executor.forward();
                isFirstAttempt = true;
            } catch (const NnTransferSocketException &e) {
                printf("ğŸš¨ Network error: %s\n", e.what());
                break;
            } catch (const NnExecutorException &e) {
                printf("ğŸš¨ Inference error: %s\n", e.what());
                break;
            }
        }
        
        // Plan ç”± unique_ptr è‡ªåŠ¨é‡Šæ”¾
    }
}